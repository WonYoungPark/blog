---
title: "Service Discovery"
categories: DevOps
tags: [DevOps, Service Discovery, Service Registry]
published: false
comments: true
---



https://botleg.com/stories/blue-green-deployment-with-docker/

https://github.com/gliderlabs/registrator

https://github.com/docker-flow/docker-flow-proxy



## 도커 스웜

### 도커 스웜에서 사용하는 용어

**스웜swarm**: 직역하면 떼, 군중이라는 뜻을 가지고 있습니다. 도커 1.12버전에서 스웜이 스웜 모드로 바뀌었지만 그냥 스웜이라고도 하는듯 합니다. 스웜 클러스터 자체도 스웜이라고 합니다. (스웜을 만들다. 스웜에 가입하다. = 클러스터를 만들다. 클러스터에 가입하다)

**노드node**: 스웜 클러스터에 속한 도커 서버의 단위입니다. 보통 한 서버에 하나의 도커데몬만 실행하므로 서버가 곧 노드라고 이해하면 됩니다. 매니저 노드와 워커 노드가 있습니다.

**매니저노드manager node**: 스웜 클러스터 상태를 관리하는 노드입니다. 매니저 노드는 곧 워커노드가 될 수 있고 스웜 명령어는 매니저 노드에서만 실행됩니다.

**워커노드worker node**: 매니저 노드의 명령을 받아 컨테이너를 생성하고 상태를 체크하는 노드입니다.

**서비스service**: 기본적인 배포 단위입니다. 하나의 서비스는 하나의 이미지를 기반으로 생성하고 동일한 컨테이너를 한개 이상 실행할 수 있습니다.

**테스크task**: 컨테이너 배포 단위입니다. 하나의 서비스는 여러개의 테스크를 실행할 수 있고 각각의 테스크가 컨테이너를 관리합니다.

### 도커 스웜이 제공하는 기능

Docker 1.13을 기준으로 어떤 기능을 제공하는지 하나하나 살펴봅니다.

**스케줄링 scheduling**: 서비스를 만들면 컨테이너를 워커노드에 배포합니다. 현재는 균등하게 배포spread하는 방식만 지원하며 추후 다른 배포 전략이 추가될 예정입니다. 노드에 라벨을 지정하여 특정 노드에만 배포할 수 있고 모든 서버에 한 대씩 배포하는 기능(Global)도 제공합니다. 서비스별로 CPU, Memory 사용량을 미리 설정할 수 있습니다.

**고가용성 High Available**: [Raft 알고리즘](https://raft.github.io/)을 이용하여 여러 개의 매니저노드를 운영할 수 있습니다. 3대를 사용하면 1대가 죽어도 클러스터는 정상적으로 동작하며 매니저 노드를 지정하는 방법은 간단하므로 쉽게 관리할 수 있습니다.

**멀티 호스트 네트워크 Multi Host Network**: Overlay network로 불리는 SDN(Software defined networks)를 지원하여 여러 노드에 분산된 컨테이너를 하나의 네트워크로 묶을수 있습니다. 컨테이너마다 독립된 IP가 생기고 서로 다른 노드에 있어도 할당된 IP로 통신할 수 있습니다. (호스트 IP를 몰라도 됩니다!)

**서비스 디스커버리 Service Discovery**: 서비스 디스커버리를 위한 자체 DNS 서버를 가지고 있습니다. 컨테이너를 생성하면 서비스명과 동일한 도메인을 등록하고 컨테이너가 멈추면 도메인을 제거합니다. 멀티 호스트 네트워크를 사용하면 여러 노드에 분산된 컨테이너가 같은 네트워크로 묶이므로 서비스 이름으로 바로 접근할 수 있습니다. Consul이나 etcd, zookeeper와 같은 외부 서비스를 사용하지 않고 어떠한 추가 작업도 필요 없습니다. 스웜이 알아서 다 해 줍니다.

**순차적 업데이트 Rolling Update**: 서비스를 새로운 이미지로 업데이트하는 경우 하나 하나 차례대로 업데이트합니다. 동시에 업데이트하는 작업의 개수와 업데이트 간격 시간을 조정할 수 있습니다.

**상태 체크 Health Check**: 서비스가 정상적으로 실행되었는지 확인하기 위해 컨테이너 실행 여부 뿐 특정 쉘 스크립크가 정상으로 실행됐는지 여부를 추가로 체크할 수 있습니다. 컨테이너 실행 여부로만 체크할 경우 아직 서버가 실행 되지 않아 접속시 오류가 날 수 있는 미묘한 타이밍을 잡는데 효과적입니다.

**비밀값 저장 Secret Management**: 비밀번호를 스웜 어딘가에 생성하고 컨테이너에서 읽을 수 있습니다. 비밀 값을 관리하기 위한 외부 서비스를 설치하지 않고 쉽게 사용할 수 있습니다.

**로깅 Logging**: 같은 노드에서 실행 중인 컨테이너뿐 아니라 다른 노드에서 실행 중인 서비스의 로그를 한곳에서 볼 수 있습니다. 특정 서비스의 로그를 보려면 어느 노드에서 실행 중인지 알필요도 없고 일일이 접속하지 않아도 됩니다.

**모니터링 Monitoring**: 리소스 모니터링 기능은 제공하지 않습니다. 3rd party 서비스([prometheus](https://prometheus.io/), [grafana](http://grafana.org/))를 사용해야 합니다. 설치는 쉽지만 은근 귀츈…

**크론, 반복작업 Cron**: 크론, 반복작업도 알아서 구현해야 합니다. 여러 가지 방식으로 해결할 수 있겠지만 직접 제공하지 않아 아쉬운 부분입니다.



### Host 변경

```bash
$ docker-machine use manager1
#or
$ eval "$(docker-machine env manager1)"

# 현재 active인 host 조회
$ docker-machine active
```



### 스웜 클러스터 생성

스웜 클러스터를 생성해 보도록 하겠습니다. 스웜 클러스터는 우선 매니저 노드를 생성하고, 매니저 노드가 생성한 토큰을 사용하여, 워커 노드에서 매니저 노드로 접속하는 방식입니다.

매니저 노드를 설정하기 위해서  `docker swarm init` 명령을 사용하면 됩니다.

```shell
$ docker swarm init --advertise-addr <MANAGER-IP>

# docker swarm init --advertise-addr $(docker-machine ip `docker-machine active`)
```

output:

```shell
Swarm initialized: current node (kgya72ho7xmhf2imwzbvbquqe) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-0q5o76buc6ptbe5skfr41k5og5tpu1bwelo61w6w15ekde429e-d8ysfo5pg8zw6nzn9nryibf1o 192.168.99.104:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

매니저 노드가 생성되면서 워커 노드에서 매니저 노드로 접속하는 명령어를 알려줍니다. 해당 명령어를 복사하였다가 추후에 워커노드에서 사용하도록 하겠습니다.

> **Tip **
>
> - ip는 `docer-machine ls` 커맨드를 입력하면 URL 컬럼에 출력됩니다.
> - 해당 토큰을 복사하지 못하였다면 매니저 노드에서 `docker swarm join-token manager` 명령를 실행하시면 매니저 노드 접속 명령어를 반환합니다.



### Node 조회

매니저 노드에서 `docker node ls` 커맨드를 수행하여 스웜에서 노드를 확인할 수 있습니다.

```shell
$ docker node ls
```



### Worker 노드 추가

```shell
$ docker-machine create worker1
$ docker-machine ssh worker1

$ docker swarm join --token SWMTKN-1-0q5o76buc6ptbe5skfr41k5og5tpu1bwelo61w6w15ekde429e-d8ysfo5pg8zw6nzn9nryibf1o 192.168.99.104:2377
```



```shell
$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
isshrd2f0ffvh3hjfvs8rfs7p *   manager1            Ready               Active              Leader              18.05.0-ce
tsgko4ffpuhzz14tfogoozogv     worker1             Ready               Active                                  18.05.0-ce
1a5gy8nbd8zlb4upz1my1v4uz     worker2             Ready               Active                                  18.05.0-ce
```



### 서비스 생성

클러스터를 구성하였으니 간단한 서비스를 생성해 보도록 하겠습니다.

서비스를 생성하려면 **매니저 노드**에서 `docker service create` 명령을 사용합니다.

```bash
$ docker service create --replicas 1 --name helloworld alpine ping docker.com
```



### 서비스 목록

생성된 서비스가 정상적으로 생성되었는지 확인해 보도록 하겠습니다.

매니저 노드에서 `docker service ls` 명령을 통해 현재 실행 중인 서비스들의 목록을 확인 할 수 있습니다.

```bash
$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
c8t242k2azjn        helloworld          replicated          1/1                 alpine:latest
```



### 서비스 상세 상태

좀 더 상세한 서비스 정보를 확인하기 위해서 `docker service ps <서비스 명>` 을 사용합니다.

해당 서비스가 어떠한 worker 노드에서 구동 중인지 확인할 수 있습니다.

```bash
$ docker service ps helloworld

ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
kxr96fj59yj3        helloworld.1        alpine:latest       manager1            Running             Running 3 minutes ago
```



```bash
$ docker service inspect helloworld --pretty

ID:		c8t242k2azjnruvosorm4rwpp
Name:		helloworld
Service Mode:	Replicated
 Replicas:	1
Placement:
UpdateConfig:
 Parallelism:	1
 On failure:	pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:	1
 On failure:	pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:		alpine:latest@sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0
 Args:		ping docker.com
Resources:
Endpoint Mode:	vip
```

> **Tip:** 스웜 명령어는 매니저 노드에서만 실행됩니다.



### Scale 조정

서비스를 운영하다 보면 트래픽에 따라서 서버를 증설해야할 경우가 발생합니다. 

도커 스웜은 간편하게 서비스를 증설할 수 있는 명령어를 지원합니다.

`docker service scale <SERVICE_NAME>=<NUMBER>` 명령을 이용하면 됩니다.

```bash
$ docker service scale helloworld=5
helloworld scaled to 5
overall progress: 2 out of 5 tasks
1/5: preparing [=================================>                 ]
2/5: preparing [=================================>                 ]
3/5: running   [==================================================>]
4/5: preparing [=================================>                 ]
5/5: running   [==================================================>]
```



`docker service ps <SERVICE_NAME>` 명령을 사용하여 상태를 확인합니다.

```
docker service ps helloworld
```



### 서비스 삭제

```bash
$ docker service rm helloworld
```



## Portainer

- Portainer는 다른 docker 환경을 쉽게 관리할 수 있는 경량 관리 UI 입니다.
- Portainer를 사용하면 쉽게 배포할 수 있습니다. docker 엔진에서 실행할 수 있는 단일 컨테이너로 구성되어있습니다.
- Portainer를 사용하면 docker 컨테이너, 이미지, 볼륨, 네트워크 등을 관리할 수 있습니다. 독립 실행형 docker 엔진 및 docker 스웜모드와 호환됩니다.



### 설치

도커 스웜 환경에서 Portainer를 사용하려면 아래 명령어를 수행하면 됩니다.

```bash
$ docker service create \
--name portainer \
--publish 9000:9000 \
--replicas=1 \
--constraint 'node.role == manager' \
--mount type=bind,src=//var/run/docker.sock,dst=/var/run/docker.sock \
portainer/portainer \
-H unix:///var/run/docker.sock
```

---



`registrator` 이미지를 사용하여 실행 중인 컨테이너를 Consul(Serivce Registry)에 등록하였습니다. consul-template는 Consul에 등록 된 실행 중인 서비스의 네트워크 주소를 이용하여 nginx의 구성파일을 생성합니다. nginx 설정 템플인 `default.ctmpl` 파일을 다음과 같이 작성합니다.



```bash
{{$blue  := env "BLUE_NAME"}}
{{$green := env "GREEN_NAME"}}
{{$live  := file "/var/live"}}
worker_processes  1;

events {
    worker_connections  1024;
}

http {
  upstream blue {
    least_conn;
    {{range service $blue}}
    server {{.Address}}:{{.Port}} max_fails=3 fail_timeout=60 weight=1;{{else}}
    server 127.0.0.1:55000;{{end}}
  }

  upstream green {
    least_conn;
    {{range service $green}}
    server {{.Address}}:{{.Port}} max_fails=3 fail_timeout=60 weight=1;{{else}}
    server 127.0.0.1:55000;{{end}}
  }

  server {
    listen 80 default;

    location / {
      {{if eq $live "blue"}}
      proxy_pass http://blue;
      {{else}}
      proxy_pass http://green;
      {{end}}
    }
  }

  server {
    listen 8080;

    location / {
      {{if eq $live "blue"}}
      proxy_pass http://green;
      {{else}}
      proxy_pass http://blue;
      {{end}}
    }
  }
}
```

위에서 작성한 nginx의 구성파일에는 다음과 같은 변수를 가지고 있습니다.

- `blue`: 환경 변수에서 가져온 Blue 배포 도커 컨테이너 이름
- `green`: 환경 변수에서 가져온 Green 배포 도커 컨테이너 이름
- `live`: 현재 실행 중인 환경. `blue` 또는 `green`



`/var/live` 파일에 현재 실행 중인 환경 값(`blue` 또는 `green`)을 저장해야 합니다.

실행 중인 도커 이미지 내부에서 환경 변수의 값을 전역적으로 변경할 수 없으므로 환경변수를 사용할 수 없습니다. 그래서 현재 라이브 환경을 작성하고, 파일로 전환해서 콘서트 내부 템플릿에서 내용을 읽습니다??



실제 환경을 전환하는 또 다른 스크립트가 필요합니다. 스크립트는 매개 변수 중 하나인 `blue` 를 받고, 현재 라이브 환경을 해당 값으로 변경합니다. 전환 스크립트 파일은 `switch.sh` 파일은 다음과 같습니다.

```bash
#!/bin/sh

if [ $# -eq 0 ]
  then
    echo "No arguments supplied"
    exit 1
fi

if [ $1 = "blue" ]
  then
    echo -n "blue" > /var/live
  else
    echo -n "green" > /var/live
fi

consul-template -consul=$CONSUL_URL -template="/templates/default.ctmpl:/etc/nginx/nginx.conf:nginx -s reload" -retry 30s -once 
```

`switch.sh` 스크립트는 전달 된 아규먼트가 없으면 오류 메시지를 출력하고 종료 합니다. 아규먼트가 `blue` 이면 `blue` 문자가  `/var/live` 파일에 작성됩니다. 아규먼트가 `green`

이면 `/var/live` 파일에 `green` 문자가 작성됩니다.

consul-template 매개변수에 `-once` 를 입력했기 때문에 템플릿이 한번 생성됩니다.



## Blue-Green 이미지

```
FROM nginx:alpine

RUN apk add --no-cache --virtual unzip
ADD https://releases.hashicorp.com/consul-template/0.14.0/consul-template_0.14.0_linux_amd64.zip /usr/bin/
RUN unzip /usr/bin/consul-template_0.14.0_linux_amd64.zip -d /usr/local/bin

COPY files/s* /bin/
RUN chmod +x /bin/switch /bin/start.sh
COPY files/default.ctmpl /templates/

ENV LIVE blue
ENV BLUE_NAME blue
ENV GREEN_NAME green

EXPOSE 80 8080
ENTRYPOINT ["/bin/start.sh"]
```

- nginx 이미즈를 base 이미지로 사용하고 있습니다.
- consul-template를 다운로드 받아 설치합니다.
- `files/start.sh`, `files/default.ctmpl` 파일을 복사합니다.
- 환경변수 기본값을 설정합니다.
  - `LIVE`: 초기 라이브 환경을 설정합니다.
  - `BLUE_NAME`: BLUE 배포 환경의 도커 컨테이너 명
  - `GREEN_NAME`: GREEN 배포 환경의 도커 컨네이너 명
- 80 포트와 8080 포트를 노출합니다.
- 이미지가 생성(또는 시작) 할때 `/bin/start.sh` 파일을 실행합니다.



## Blue-Green 배포

`Blue-Green 배포` 를 진행하려면 다음의 `docker-compose.yml` 파일을 사용합니다.

```yaml
version: '2'

services:
  bg:
    image: hanzel/blue-green
    container_name: bg
    ports:
      - "80:80"
      - "8080:8080"
    environment:
      - constraint:node==master
      - CONSUL_URL=${KV_IP}:8500
      - BLUE_NAME=blue
      - GREEN_NAME=green
      - LIVE=blue
    depends_on:
      - green
      - blue
    networks:
      - blue-green

  blue:
    image: hanzel/nginx-html:1
    ports:
      - "80"
    environment:
      - SERVICE_80_NAME=blue
    networks:
      - blue-green

  green:
    image: hanzel/nginx-html:2
    ports:
      - "80"
    environment:
      - SERVICE_80_NAME=green
    networks:
      - blue-green

networks:
  blue-green:
    driver: overlay
```



### 서비스 시작

```bash
$ docker-compose up -d
```



```bash
$ docker-compose scale blue=3 green=3
```



```bash
$ docker-compose ps
```



### 전환

```bash
$ docker exec bg switch green
```



### 현재 라이브 환경 확인

다음의 명령을 통해서 현재 라이브 환경을 확인할 수 있습니다.

```bash
$ docker exec bg cat /var/live
```

